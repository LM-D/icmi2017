{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"imgs/header.png\">\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Probabilistic filtering for input recognition\n",
    "#### Inferring user intention in a noisy world\n",
    "<b>[John Williamson](http://johnhw.com)</b> \n",
    "\n",
    "----\n",
    "\n",
    "    All theorems are true. \n",
    "    All models are wrong. \n",
    "    And all data are inaccurate. \n",
    "\n",
    "    What are we to do? \n",
    "    We must be sure to remain uncertain.\n",
    "\n",
    "-- *[Leonard A. Smith, Proc. International School of Physics ``Enrico Fermi\", (1997)](http://www2.maths.ox.ac.uk/~lenny/fermi96_main_abs.html)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "-----------------\n",
    "### What are we going to do?\n",
    "We will:\n",
    "* show how to represent interaction problems as inference;\n",
    "* discuss how probabilistic filters can be used to attack these inference problems;\n",
    "* specifically show how motion-based interfaces can use probabilistic filtering to increase robustness.\n",
    "\n",
    "\n",
    "### What will we *practically* do?\n",
    "* We will build a 2D mouse gesture recognizer using a hybrid discrete/continuous particle filter. This will be a simple, robust classifier with rich feedback opportunities.\n",
    "\n",
    "<img  src=\"imgs/capture.png\" width=\"80%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#### What is probabilistic filtering ?\n",
    "One view on interaction is to see user intentions as **unknown values** which are partially observed through input sensors. The time series of inputs from the user only give a partial, noisy, incomplete view of intention inside the user's head. \n",
    "\n",
    "Probabilistic filtering **(PF)** tracks the evolution of some unknown variables *[user intentions]* given observed evidence *[user input]*, in a way that is **robust**. Probabilistic filters infer a **distribution** over possible hidden (unobserved) variables, updating them over time. These filters are inherently **uncertain**, as they represent degrees of belief, and **dynamic**, as they explicitly model changing state over time.\n",
    "\n",
    "<img src=\"imgs/brain_inference.png\">\n",
    "\n",
    "#### Simulation viewpoint\n",
    "These filters are really *simulators*. They *simulate* how possible user behaviors might unfold over time. In some probabilistic filters, hundreds of parallel simulators are run, each with slightly different parameters. In all cases, the simulations are adjusted online to better match observed reality. The internal parameters that drive the simulation are the *unknown variables* we want to infer and the *evidence* is the observed reality that adjusts the simulation parameters.\n",
    "\n",
    "#### Properties\n",
    "Probabilistic filtering is:\n",
    "\n",
    "| Property | Why  |\n",
    "|----------|------|\n",
    "|**Bayesian**  |  Represents degrees of belief using probability distributions.    |\n",
    "|**predictive**  |  Works by comparing predictions with reality.   |\n",
    "|**generative** |  Involves generating (i.e. simulating) behavior.   |\n",
    "\n",
    "-----\n",
    "Probabilistic filtering is an **inverse probability** approach, and it requires that we think of interaction from an unique perspective. We have to explicitly be able to write down:\n",
    "\n",
    "* what we want to know (i.e. the **state space of intention**);\n",
    "* how that will change over time (i.e. the **dynamics of intention**);\n",
    "*  a model that *if we knew what the user intention was, what the expected behavior would be* (i.e. a **generative function mapping intention -> expected user inputs**).\n",
    "\n",
    "Note that this last point is the **inverse** of the typical way of approaching this problem, where we would try and find a mapping from a sensors to intention, by design or by learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this computational HCI?\n",
    "Probabilistic filtering means writing down an **executable, statistical model** of user behavior, then **running an inference algorithm** that updates beliefs based on the way observations evolve. The **parameters** of the filter can be **learned from user data**. The effectiveness of the filter can be quantitatively measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are competitive approaches?\n",
    "#### **Crafted mappings**\n",
    "**where we try to find (by hand) transforms from sensors to intentions that are  simple or obvious.**\n",
    "\n",
    "**Example:** a button, which has two physical states, and maps on to two intentional states via two electrical states. Pushed down = current flows = user intended to switch on. The mapping from electrical states to intentional states is **designed.**\n",
    "<img src=\"imgs/undo.jpg\">\n",
    "*[Image credit: David Singleton via flickr.com CC-BY 2.0]*\n",
    "\n",
    "#### **Machine learned mappings**\n",
    "**where we train a system to recognize a class of input patterns as being representative of an intended behavior. **\n",
    "**Example:** Finger gesture recognizer; hundreds of examples of many users performing one of N multi-touch gestures are recorded. These are used to train a random forest to classify the intended gesture. The mapping from electrical states (capacitive sensors) to intentional states is **learned**.\n",
    "\n",
    "<img src=\"imgs/svm.jpg\" width=\"300px\">\n",
    "*[Image credit: Elisfm - via Wikimedia Commons; public domain]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "* **Robustness to noise** PFs work well even with input sensors that are noisy.\n",
    "* **Robustness to poorly specified models** PFs can cope predictably even if our models are bad.\n",
    "* **Robustness to intermittence** PFs can continue to sensibly interpolate when input cuts out.\n",
    "* **Uncertainty estimates** PFs *know how certain they are* and this can be used in the interaction design.\n",
    "* **Decoupled from real-time** PFs can infer past (smoothing), present (filtering) and future (forecasting).\n",
    "* **Inherent fusion of multiple input sensors** PFs are often used to solely to fuse together multiple inputs from different sensors.\n",
    "* **Better feedback** PFs  offer the opportunity to give users rich insight into the process of intention decoding.\n",
    "* **Flexible modeling** PFs can incorporate both fundamental modeling (e.g. physiological or cognitive models) and data-driven machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles \n",
    "-------\n",
    "\n",
    "> Interaction is the process of driving a system into a state compatible with user intentions.\n",
    "\n",
    "There are many perspectives on interaction from this stance, including:\n",
    "\n",
    "| Perspective   | Burden | Characteristic                         |\n",
    "|---------------|--------|----------------------------------------|\n",
    "| Communication | User   | User gets information into the system, by encoding intentions. |\n",
    "| Control       | Split  | User drives state towards intention via feedback control.   |\n",
    "| Inference     | System | System infers what user intention is from sensed user actions. |\n",
    "\n",
    "### Interaction as inference\n",
    "If we view interaction as inference of intention, there are three elements:\n",
    "* **Interaction is inference**; it is the process of inferring a hidden variable: what the user wants a system to do. \n",
    "* **Observations are noisy and incomplete** What a system sees is a distorted and incomplete representation of user actions in the world, which are in turn a noisy representation of internal intentions (your hand does not always go where you want it...)\n",
    "* **Interaction occurs over time** Interaction is a *process* that evolves over time. Information flow is not instantaneous.\n",
    "\n",
    "<img src=\"imgs/brainspace.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview diagram\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/control_loop.png\">\n",
    "\n",
    "\n",
    "\n",
    "Notation:\n",
    "* We have a sequence of states over time, indexed by $t$\n",
    "* $X_t$ the variable we want to know (at time $t$) (e.g. an intention inside a user's head). \n",
    "* $Y_t$ the variable we can observe (e.g. a sensor we can get readings from).\n",
    "* For computational simplicity, we assume **discrete time**, i.e. we observe sensors in a discrete, regularly sampled way.\n",
    "\n",
    "* We want to compute $P(X_t|Y_t)$ (the **inverse problem**). \n",
    "* We use a **forward model** $P(Y_t|X_t)$ to infer this.\n",
    "* We need to define two functions: ${\\bf\\hat{y_t}} = f({\\bf \\hat{x}}_t)$ (the **observation function**) and $\\hat{\\bf x}_{t} = g(\\hat{\\bf x}_{t-1})$ (the **dynamics** or **process function**).\n",
    "* We also need a **weighting function** $w(\\bf\\hat{y_t},{\\bf y_t})$ that computes how similar a simulated observation $\\bf\\hat{y_t}$ is to the real observation $\\bf y_t$. This is used to compute the likelihood $p(\\bf\\hat{y_t}|{\\bf y_t})$.\n",
    "\n",
    "* $f$, $g$ and $w$ are often very simple functions.\n",
    "\n",
    "<img src=\"imgs/stochastic.png\" width=\"75%\">\n",
    "\n",
    "#### Predictor-corrector\n",
    "**This is a predictor-corrector model**; the dynamics model supplies predictions, and corrections to those predictions are applied by the observation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive filtering\n",
    "\n",
    "<img src=\"imgs/recursive.png\">\n",
    "\n",
    "Probabilistic filters are sometimes called **recursive Bayesian filters**. \n",
    "* They are **Bayesian** because they represent belief about states via probability distributions.\n",
    "* They are **recursive** because they take a *prior*, condition on *evidence* and compute a *posterior*; this *posterior* then becomes the *prior* at the next time step.\n",
    "\n",
    "As well as straightforward conditioning on observed evidence, probabilistic filters incorporate dynamics which form predictions of the world at the next time step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem description\n",
    "We are going to solve a simple problem:\n",
    "\n",
    "* Recognising simple 2D gestures, drawn with a mouse.\n",
    "\n",
    "This is readily solved with \"standard\" (inverse) algorithms, but we will show how a model-led approach lets us encode our assumptions elegantly **and** we get all the benefits of *probabilistic* tracking. We'll see how probabilistic filters degrade gracefully when our models are bad or measurements are noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Filtering \n",
    "Particle filters are very simple to understand:\n",
    "\n",
    "* **Sample-based:** Instead of trying to represent the PDFs of our distributions, which could be any function that integrates to unity, we represent **distributions** using **samples** from those distributions (i.e. it is a **sequential Monte Carlo** method; we use samples to represent distributions). We choose some fixed number of particles $N_s$ and have a set of particles $S_t = \\{ x^{1}_t, x^{2}_t, \\dots, x^{N_s}_t \\}$. At any time point, our belief is captured by this sample set.\n",
    "* **Arbitrary dynamics:** we can apply **any** dynamics simply by applying our transition function $f({\\bf x_t})$ to each sample from our current set $S_t$ to get a new predicted set of samples $S_{t+1}$. Likewise, for any sample, we can compute the corresponding observation by applying an arbitrary function $g(x_t)$ to each sample independently. There are **no** restrictions on the form of $f({\\bf x_t})$ and $g({\\bf x_t})$.\n",
    "* **Arbitrary distribution:** Our uncertainty is captured by the locations of the set of particles in the state space; they are samples from our current posterior. We do not have an explicit parametric form for the distribution; samples are all that we have. Therefore there are no restrictions on the form of the distribution; it can be multi-modal, heavy-tailed etc. \n",
    "* **No explicit likelihood:** We don't explicitly compute the likelihood of observations, but instead we apply some weighting function to the particles and then normalise these weights to approximate the likelihood. This means we only have to find a reasonable weighting function, and not a true likelihood.\n",
    "* **Importance sampled** We **resample** particles which are likely after an observation, and discard those which are unlikely. This is **importance resampling**. Without this step, particles would quickly diffuse into very unlikely parts of the space. **Importance resampling** continuously adapts the particles to cover likely portions of the space.\n",
    "\n",
    "**This can only ever approximate the \"true\" distribution; but the Monte Carlo approximation turns out to work surprisingly well, and has good theoretical guarantees. **\n",
    "\n",
    "<img src=\"imgs/particleprocess.png\">\n",
    "\n",
    "\n",
    "It might seem surprising that would work, but in fact for many problems it works very well, and it is statistically sound (under relatively relaxed conditions). It is less computationally efficient than the Kalman filter, and also less inferentially efficient (it is usually less certain for the same amount of data). \n",
    "\n",
    "It has many, many variants and many parameters that can be tweaked. This is flexible, but can be hard to optimize. \n",
    "\n",
    "However, it is very easy to implement and its form makes it easy to adapt into other parts of an interface. For example, to compute how likely it is that a user wants to activate a target (in a cursor tracking example) we can simply count all of the particles that fall into the target bounding box; no integration required!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particle filters have many uses in HCI. For example, we have used them extensively to track finger configurations when using capacitive sensors. In this case, we have a finger pose state space (hidden) and a sensor matrix (observed), and the filter estimates pose in real-time.\n",
    "\n",
    "<img src=\"imgs/finger_track.png\">\n",
    "\n",
    "<img src=\"imgs/anglepose.jpg\">\n",
    "[See the AnglePose video](http://www.dcs.gla.ac.uk/~jhw/AnglePose-final.mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "We will use the **particle filter** algorithm (technically the **SIR** variant, which is the simplest to understand).\n",
    "\n",
    "A particle filter requires that we specify:\n",
    "* A **dynamics function** that predicts how we expect the world to evolve, which takes \n",
    "$\\hat{\\bf{x}}_t \\rightarrow \\hat{\\bf x}_{t+1}$\n",
    "* An **observation function** that predicts what we expect to observe, given a hypothesized state $\\hat{\\bf y}_t \\rightarrow \\hat{\\bf{x}_t}$\n",
    "* A **weight function**, that, given a hypothesized observation $\\hat{\\bf y}_t$, can be used to compute $p(\\hat{\\bf y}_t|{\\bf y}_t)$. This is performed by computing weights $w_i$ for each particle $i$ and then normalizing to produce a probability:\n",
    "$$p^{(i)}(\\hat{\\bf y}_t|{\\bf y}_t) = \\frac{w_i}{\\sum_j w_j}$$\n",
    "* A set of **prior distributions** that specify our initial guesses for $\\hat{\\bf x}_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why particle filter?\n",
    "The Kalman filter uses the normal distribution to model all of the uncertainty in the system. This is great for computational efficiency, since the updates are simple linear transforms. It is also inferentially efficient; given only a small amount of evidence the Kalman filter will converge quickly compared to other approaches.\n",
    "\n",
    "But it has several significant drawbacks, which make it difficult to apply directly to infer *gestures* from observations:\n",
    "\n",
    "#### Kalman filter drawbacks\n",
    "* the **dynamics** have to be linear: we can't have complicated dynamic models (although we can linearise at each time step).\n",
    "\n",
    "This doesn't make much sense for tracking complex gesture trajectories; a dynamic model for a complete gesture is rarely going to be linear. We might want to be able to learn complex dynamics using a deep network, for example, and then plug them into a probabilistic filter. A Kalman filter does not support this.\n",
    "\n",
    "* all of the **uncertainty** must be normal: so we can't track multiple modes, for example, because a normal distribution has exactly one mode. \n",
    "\n",
    "Imagine an object disappearing behind an obstruction which could reappear on either side; the Kalman filter can only spread out the distribution over the whole area, with an expected location in the middle of the obstacle! We would like to instead be able to track the two possibilities here by splitting up the hypotheses. \n",
    "\n",
    "<img src=\"imgs/landscape.png\">\n",
    "*[Waddington's epigenetic landscape, illustrating a dynamic system which develops multiple modes as it evolves; a Gaussian approximation is wholly inappropriate]*\n",
    "\n",
    "Very often in HCI we encounter problems with a combination of discrete variables and continuous ones.\n",
    "This is critical in gesture recognition for example; at any point in time, our hypotheses might be split among multiple possible gestures with different spatial distributions. Being able to represent the combination of discrete + continuous states is critical. (**Kalman filter banks** are an alternative approach, which explicitly maintain the competing hypotheses as separate Kalman filters).\n",
    "\n",
    "As an aside, the **hidden Markov model**, formerly a key algorithm in speech recognition, is to discrete state tracking what the Kalman filter is to continuous state tracking. The HMM can track discrete hidden states easily (with discrete or continuous observation space), but cannot track continuous variables on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic filtering\n",
    "We will first implement a basic particle filter that can track a very simple one dimensional time series. This toy problem is easy to understand and work with.\n",
    "\n",
    "Then, we will show how this can generalise to an interesting HCI problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the things we need\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "import pfilter\n",
    "import ipywidgets\n",
    "import IPython\n",
    "import matplotlib, matplotlib.colors\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(8.0, 4.0), dpi=140)\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test data\n",
    "To test the particle filter, we will try and track a very simple, 1D sine wave:\n",
    "$$Y_t=\\sin(t)$$\n",
    "\n",
    "This is a simple, smooth process.\n",
    "We will try and estimate our \"hidden\" value $\\bf{X}$ via an observed variable $\\bf{Y}$ which is just $\\bf X$ with some noise added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0,20,100).reshape(-1,1)\n",
    "x = np.sin(t)\n",
    "plt.plot(t,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model\n",
    "We will use a very simple model\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that there are no predictable dynamics, just some Gaussian noise $X_t = X_{t+1} + \\epsilon,\\  \\epsilon \\sim N(0,\\sigma_p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_p = 0.2              # the process noise \n",
    "beta = 0.25                # the RBF width\n",
    "simulated_noise = -2     # the simulated noise we added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identity Example\n",
    "def dynamics(x):\n",
    "    # tomorrow is the same as today\n",
    "    # but slightly randomly different\n",
    "    # we literally *add* noise to our previous state\n",
    "    return x+np.random.normal(0,sigma_p,x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show what this dynamics model looks like, if we had no resampling step in the filter. This makes predictions without ever having seen any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_dynamics(priors, dynamics, steps, n_runs=1):\n",
    "    runs = []\n",
    "    def simulate_run():\n",
    "        x = np.array([p.rvs() for p in priors])\n",
    "        xs = [x]    \n",
    "        for i in range(steps):\n",
    "            x = dynamics(x)\n",
    "            xs.append(x)\n",
    "        return xs    \n",
    "    return np.array([simulate_run() for j in range(n_runs)])\n",
    "    \n",
    "simulated = simulate_dynamics(priors=[norm(0,1)], dynamics=dynamics, steps=200, n_runs=20)   \n",
    "plt.plot(simulated[:,:,0].T, alpha=0.3, c='C0');\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"X\")\n",
    "plt.title(\"Simulated runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Observation**\n",
    "We assume that the sensor we measure is the value we want to infer, i.e. $Y_t=X_t$, and $g(X_t)$  is just the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(x):\n",
    "    # we observe x directly\n",
    "    return x[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Weighting**\n",
    "We weight samples according to how similar they are to the observed output. We use a simple **heat kernel**:\n",
    "$$w_i = e^{\\left(-\\frac{(y-y^\\prime)^2}{2\\beta^2}\\right)}$$\n",
    "$\\beta$ is a parameter that lets us specify how precise our matching between observation and reality is.\n",
    "\n",
    "Note that this gives more weight to particles that are more similar to the observation: it is a **similarity** function, not a distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(true_y, hypothesized_y):\n",
    "    # RBF similarity function    \n",
    "    return np.exp(-np.sum((true_y-hypothesized_y)**2, axis=0)/(0.5*beta**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what this function looks like, comparing a true value of 0 with values in [-1, 1], $\\beta=0.25$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo = np.linspace(-1,1,100).reshape(1,100)\n",
    "plt.plot(hypo.reshape(100,), weight(0, hypo))\n",
    "plt.axvline(0,c='C1')\n",
    "plt.xlabel(\"Hypothesized value\")\n",
    "plt.ylabel(\"Unnormalised weight\")\n",
    "plt.title(\"RBF kernel, $\\\\beta$=0.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Priors**\n",
    "We assume a very simple prior on $X$; that it is normally distributed, mean 0, variance 1, $X_0 \\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume that, before seeing any evidence, that the particles are \n",
    "# normally distributed about 0, with std. dev. 1.0\n",
    "prior = [norm(0,1)] # x ~ N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_simple = pfilter.ParticleFilter(initial=prior, \n",
    "                                    observe_fn=observe,\n",
    "                                    n_particles=200,                                    \n",
    "                                    dynamics_fn=dynamics,\n",
    "                                    weight_fn=weight,                    \n",
    "                                    resample_proportion=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pfilter(pfilter, inputs):\n",
    "    \"\"\"Apply a particle filter to a time series of inputs,\n",
    "    and return the particles, their weights, and the mean\n",
    "    estimated state\"\"\"    \n",
    "    # reset filter\n",
    "    pfilter.init_filter()\n",
    "    particles = []\n",
    "    weights = []\n",
    "    means = []    \n",
    "    # apply to each element of the time series\n",
    "    for i in range(len(inputs)):    \n",
    "        pfilter.update([inputs[i]])\n",
    "        particles.append(pfilter.particles)    \n",
    "        weights.append(pfilter.weights)\n",
    "        means.append(pfilter.mean_state)        \n",
    "    return np.array(particles), np.array(weights), np.array(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a utility function to plot the results of running a particle filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pfilter(time,expected, observed, particles, weights, means):\n",
    "    \"\"\"Apply a particle filter to a time series, and plot the\n",
    "    first component of the predictions alongside the expected\n",
    "    output.\"\"\"\n",
    "    # expected output\n",
    "    plt.plot(time, expected, 'C1', lw=3)\n",
    "    plt.plot(time, observed, '+C3', lw=3)\n",
    "    \n",
    "    # particles \n",
    "    ts = np.tile(time[:,None], particles.shape[1]).ravel()\n",
    "    weights =  weights.ravel()    \n",
    "    rgba_colors = np.zeros((len(weights),4))\n",
    "    rgba_colors[:,0:3] = matplotlib.colors.to_rgb('C2')\n",
    "    weights *= 10\n",
    "    rgba_colors[:, 3] = np.clip(weights, 0, 1)\n",
    "    plt.scatter(ts, particles[:,:,0].ravel(),  c=rgba_colors, s=2)\n",
    "    # mean estimation\n",
    "    plt.plot(time, means, 'C0--', lw=2)\n",
    "    # legend\n",
    "    plt.legend([\"True\", \"Observed\", \"Mean estimate\", \"Particle\"])\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"X\")\n",
    "    plt.title(\"Particle filter estimate\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_filter(sig=0.1, bet=0.5, noise=-2):\n",
    "    global sigma_p, beta, noise_level\n",
    "    sigma_p = sig\n",
    "    beta = bet\n",
    "    noise_level = 10**noise\n",
    "    noise = np.random.normal(0,noise_level, x.shape)\n",
    "    plt.figure(figsize=(12,7))\n",
    "    y = x + noise\n",
    "    particles, weights, means = run_pfilter(pf_simple, y)\n",
    "    plot_pfilter(t, x, y, particles, weights, means)\n",
    "    plt.title(\"Sigma=%.2f, Beta=%.2f, Noise=%.2e\" % (sigma_p, beta, noise_level))\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    \n",
    "run_filter(sig=0.1, bet=0.5, noise=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adjust these interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipywidgets.interact_manual(run_filter, sig=(0.0, 2, 0.05), bet=(0.1, 5.0, 0.1), noise = (-5.0, 1.0, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to note\n",
    "* This is a trivial model, but still tracks \"complex\" functions, because it is adapting to observations\n",
    "* Things to adjust:\n",
    "    * noise level\n",
    "    * rbf width beta\n",
    "    * dynamics noise sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more interesting example\n",
    "Imagine we wanted to infer the **phase** of the oscillator driving this sine wave. The phase variable is not observable, but we can  infer it from the observed oscillation. Furthermore, we want the *unwrapped* phase, i.e. we expect the phase to monotonically increase.\n",
    "\n",
    "<img src=\"imgs/phase.gif\">\n",
    "*[Image credit:1ucasvb]*\n",
    "\n",
    "We can encode these assumptions in our model, then see if the particle filter is able to infer the hidden parameter over time.\n",
    "\n",
    "\n",
    "* **Observation**\n",
    "We postulate an observation model:\n",
    "$${\\bf y_t} = \\sin({\\bf x_t})$$\n",
    "i.e. that what we see is the effect of sine on a hidden variable $\\bf x_t$.\n",
    "Because we defined ${\\bf y_t}=\\sin(t)$, we are actually trying to infer $t$.\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that we have a very simple dynamical system in discrete time, where we have the phase and its first time derivative.\n",
    "$${\\bf x_t} = \\begin{bmatrix}x \\\\ \\dot{x}\\end{bmatrix},$$ and \n",
    "$${\\bf x_{t+1}} = {\\bf x_t} + \\begin{bmatrix}\\dot{x} \\\\ 0 \\end{bmatrix} + N(0, \\Sigma),$$ where $$\\Sigma=\n",
    "\\begin{bmatrix}\n",
    "\\sigma_x & 0 \\\\\n",
    "0 & \\sigma_dx \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This means that if we start linearly increasing or decreasing at a certain rate, we should expect to keep doing so.\n",
    "\n",
    "* **Priors**\n",
    "We again assume that the initial distribution is normally distributed, with \n",
    "$$\n",
    "{\\bf x_0} \\sim N(0, \\Sigma_0)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example\n",
    "sigma_x = 0.1\n",
    "sigma_dx = 0.001\n",
    "process_sigmas = [sigma_x, sigma_dx] # how much noise for x and dx    \n",
    "\n",
    "# transition function\n",
    "def linear_dynamics(x):        \n",
    "    nx = np.dot(x,np.array([[1,0],\n",
    "                            [1,1]]))        \n",
    "    nx += np.random.normal(0,process_sigmas,x.shape)\n",
    "    return nx\n",
    "\n",
    "def observe_sin(x):    \n",
    "    # y_t = sin(x_t[0])    \n",
    "    return np.sin(x[:,0:1])\n",
    "\n",
    "def weight_sin(hypothesized_y, true_y):\n",
    "    # RBF similarity function, as we used before     \n",
    "    w = np.exp(-np.sum((hypothesized_y-true_y)**2, axis=1)/(0.5*beta**2))    \n",
    "    return w   \n",
    "\n",
    "def prior_sin(n):\n",
    "    # simple normal prior on the initial state\n",
    "    return np.stack([scipy.stats.norm(0,1).rvs(n), scipy.stats.norm(0, 0.25).rvs(n)]).T  \n",
    "\n",
    "# drop some of the data\n",
    "pfilter = reload(pfilter)\n",
    "pf_sin = pfilter.ParticleFilter(initial=prior_sin, \n",
    "                                observe_fn=observe_sin,\n",
    "                                n_particles=200,\n",
    "                                dynamics_fn=linear_dynamics,\n",
    "                                weight_fn=weight_sin,                    \n",
    "                                resample_proportion=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_sinfilter(sig_x=0.1, sig_dx=0.01, bet=0.25, noise=-2):\n",
    "    global sigma_p, beta, noise_level, frame\n",
    "    sigma_x = sig_x\n",
    "    sigma_dx = sig_dx\n",
    "    beta = bet\n",
    "    noise_level = 10**noise\n",
    "    noise = np.random.normal(0,noise_level, x.shape)\n",
    "    plt.figure(figsize=(12,7))\n",
    "    y = x + noise\n",
    "    y[20:40,:] = np.nan\n",
    "    frame = 0\n",
    "    particles, weights, means = run_pfilter(pf_sin, y)\n",
    "    plot_pfilter(t, t, y, particles, weights, means)\n",
    "    plt.title(\"Sigma_x=%.2f, Sigma_dx=%.2f, Beta=%.2f, Noise=%.2e\" % (sigma_x, sigma_dx, beta, noise_level))\n",
    "    \n",
    "ipywidgets.interact_manual(run_sinfilter, sig_x=(0.0, 2, 0.05), sig_dx=(0.0, 0.5, 0.01), \n",
    "                           bet=(0.1, 5.0, 0.1), noise = (-5.0, 1.0, 0.1))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to note\n",
    "* The particle filter was able to infer the hidden state, despite only having a forward model (i.e knowledge of $\\sin(x)$, **not** $\\sin^{-1}(x)$)\n",
    "* It correctly unwrapped phase, because we primed it with the dynamics model to expect values that would increase at a linear rate.\n",
    "* This problem results in *multimodal* distributions, because there are an infinite number of solutions to $y=sin(x)$ because we can add any multiple of $2\\pi$ without changing anything.\n",
    "We can see these as fainter lines on the particle plot.\n",
    "* This means that the particle mean is not actually a good estimate in this case! A better choice might be the most likely particle -- the Maximum A Posteriori (MAP) estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "One element we have not discussed is the `resample_proportion=0.01` parameter when creating the particle filter.\n",
    "\n",
    "This tells the filter to replace 1% of particles in each time step with random draws from the original prior. There are formulations of the particle filter which omit this, but it is often very useful to introduce this resampling process. \n",
    "\n",
    "If the filter drifts far from the observations, or a step change occurs in the observation (imagine a camera suddenly changing exposure), it may take a long time for the particle filter to adapt. In the worst case, it might never be able to move into the relevant portion of the state space. Prior resampling helps keep a \"bank\" of fresh particles which can rapidly lock onto to new hypotheses.\n",
    "\n",
    "This is very similar in motivation to the use of multiple chains or multiple restarts in MCMC sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Gesture recognition\n",
    "Lets now apply these ideas to a practical HCI task: recognising 2D gestures. We will try and  do the full set of gesture recognition tasks is one single, probabilistic model:\n",
    "* **spot** gestures: determine when they start and end, without any external segmentation cue like a button push.\n",
    "* **recognise** gestures: label them according to class\n",
    "* **parameterise** gestures: recover parameters like the size, speed or rotation of the gestures performed.\n",
    "\n",
    "This is a challenging task! We will assume a small set of Graffiti-like symbols as the basis for this example.\n",
    "\n",
    "We will base our algorithm directly on the one given in [A Probabilistic Framework for Matching\n",
    "Temporal Trajectories](http://www.cs.toronto.edu/~jepson/papers/BlackJepsonECCV1998.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We have some example data with a few example 2D mouse-drawn gestures. Here we are assuming just a **single** template for each gesture, for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gestures\n",
    "import imp; imp.reload(gestures)\n",
    "g = gestures.GestureData(\"gestures.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture shapes \n",
    "We can plot the shapes of the gesture trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gestures = g.n_gestures\n",
    "for i in range(n_gestures):\n",
    "    plt.subplot(1, n_gestures,i+1)\n",
    "    path = g.gestures[i]    \n",
    "    plt.plot(path[:,0], path[:,1])\n",
    "    plt.text(0,0,\"%d\"%i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.axis(\"equal\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries view\n",
    "We can also see each gesture as a trajectory of two coordinates ($x,y$ coordinates) over time. This is closer to the way in which the matching will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(n_gestures):\n",
    "    plt.subplot(n_gestures,2,i*2+1)    \n",
    "    path = g.gestures[i]    \n",
    "    plt.plot(path[:,0], '-C0')\n",
    "    plt.plot(path[:,1], '-C1')\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.subplot(n_gestures,2,i*2+2)\n",
    "    plt.plot(path[:,0], path[:,1], 'C0')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis(\"equal\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_template\n",
    "We have a simple utility function `get_template(i, t)` which returns the $x,y$ co-ordinate for gesture $i$ at sample $t$. It automatically clips $t$ from 0 to the length of the gesture (in frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test get_template\n",
    "for i in range(100):\n",
    "    xy = g.get_template(1, i)\n",
    "    plt.plot(xy[0], -xy[1], 'C1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "We want to recognize 2D gestures drawn with a mouse (or finger/stylus). \n",
    "\n",
    "### We know:\n",
    "* We *observe* sequences of $x,y$ coordinates over time.\n",
    "* We have some example templates for particular shapes that we want to match (e.g. letters)\n",
    "\n",
    "### We don't know:\n",
    "* where the user will start drawing a gesture\n",
    "* how big the gesture will be    \n",
    "* how fast the user will draw the gesture (it may well be drawn at a non-constant speed)\n",
    "\n",
    "### We want to know:\n",
    "* which gesture the user is performing, if any\n",
    "* when the user has finished doing the gesture\n",
    "* the parameters of the movement, like speed of performance\n",
    "\n",
    "----- \n",
    "    \n",
    "#### Probabilistic view\n",
    "Putting this into the probabilistic framework, we want to infer a probability distribution over gesture classes and gesture completion state, given a time series of $x,y$ coordinates. The $x,y$ points form our observation vector $Y$.\n",
    "\n",
    "We assume gesture reproduction is in some way a \"noisy reproduction\" of the ideal template form, where there are various types of distortion that can be encountered.\n",
    "\n",
    "#### Markov approximation\n",
    "We could look at the whole time series of $x,y$ points and try and classify that. However, there are two problems:\n",
    "* What is the \"whole\" time series -- i.e. how do we segment the gesture?\n",
    "* We would have to store the entire series and somehow match it against templates.\n",
    "\n",
    "A simple way to eliminate these problems is to rewrite the recognizer so that it depends on nothing but its immediately previous state; i.e. so that it satisfies the Markov property.\n",
    "\n",
    "To do this, we need to introduce additional variables into the state we are estimating; but with judicious choice these can be a very small number of additional variables. In particular, we can just track how far along a gesture we are (the \"phase\") and update that over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "We are now in a position to write down a model for our gesture recognizer.\n",
    "\n",
    "### State\n",
    "First of all, the state we are trying to infer:\n",
    "\n",
    "<img src=\"imgs/gesture.png\">\n",
    "\n",
    "$$X = [i,s,x_c,y_c,\\theta,\\phi,\\dot{\\phi}]$$\n",
    "\n",
    "We have one of $n$ possible gestures\n",
    "* $i$ the number of the gesture\n",
    "\n",
    "Our model says a gesture will be identical to the template for that class of gesture, but might vary in:\n",
    "* $s$ overall scale, within some tolerance\n",
    "* $x_c,y_c$ center position (could be anywhere on screen)\n",
    "* $\\theta$ small changes in rotation (e.g. $<45^o$)\n",
    "\n",
    "We must take note that what we observe is a position at a **single time point** in a gesture. This means we must estimate how far through a gesture we are.\n",
    "* $\\phi$ the proportion of gesture complete, in the fraction [0,1].\n",
    "* $\\dot{\\phi}$ the rate at which the gesture is being performed (i.e. fast or slow).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "* Given a gesture $i$, we have a template $G_i(\\phi)$, which is returns an $x,y$ point for any value of $\\phi$.\n",
    "\n",
    "* We expect to observe $\\hat{{\\bf y}}=AG_i(\\phi)$, where $A$ is a transformation matrix applying the translation $x_c,y_c$, the scaling $s$ and the rotation $\\theta$.\n",
    "\n",
    "* The utility function `linear_transform()` defined below is a useful component in implementing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def linear_transform(xys, angle=0.0, scale=1.0, translate=(0,0)):\n",
    "    \"\"\"Takes a an n x 2 array of point `xys` and returns the 2D points transformed by\n",
    "    rotating by `angle` (degrees)\n",
    "    scaling by `scale` (proportional 1.0=no change, 0.5=half, etc.)\n",
    "    translating by `translate` ((x,y) offset)\"\"\"\n",
    "    ca, sa = np.cos(np.radians(angle)), np.sin(np.radians(angle))\n",
    "    rot = np.array([[ca, -sa], \n",
    "                    [sa, ca]])\n",
    "    return np.dot(xys, rot)*scale + np.array(translate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo of linear transform on points distributed in a unit square\n",
    "original = np.random.uniform(0,1, size=(200,2))\n",
    "transformed = linear_transform(original, angle=45, scale=1.5, translate=(-5,2))\n",
    "plt.plot(transformed[:,0], transformed[:,1], '.')\n",
    "plt.plot(original[:,0], original[:,1], '.')\n",
    "plt.axis(\"square\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gesture_observation(state):\n",
    "    # given an n x d matrix of n particle samples\n",
    "    # return a n x 2 matrix of expected x,y, positions for that gesture model\n",
    "    # dummy code,which returns random results:\n",
    "  \n",
    "    transformed = [linear_transform(g.get_template(s[0], s[5]), scale=s[1], angle=0, \n",
    "                                      translate=[s[2], s[3]]) for s in state]                 \n",
    "    \n",
    "    #transformed = [linear_transform(g.get_template(s[0], s[5]), scale=1, angle=0, \n",
    "    #                                 translate=[s[2], s[3]]) for s in state]                 \n",
    "    \n",
    "    \n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamics\n",
    "We then specify some simple dynamics. These all allow the values to slowly change over time (i.e. some random drift), except for the phase $\\phi$ which we also expect to steadily increase at the rate $\\dot{\\phi}$.\n",
    "\n",
    "Specifically:\n",
    "* $s_{t+1} = s_{t} + \\sigma_s$ scale can drift slowly\n",
    "* $x_{t+1} = x_{t} + \\sigma_x$ position can drift slowly\n",
    "* $y_{t+1} = y_{t} + \\sigma_y$ position can drift slowly\n",
    "* $\\theta_{t+1} = \\theta_{t} + \\sigma_\\theta$ orientation can drift slowly\n",
    "* $i_{t+1}=i_{t}$  gesture class never changes\n",
    "* $\\phi_{t+1} = \\phi_{t} + \\dot{\\phi}_{t} + \\sigma_y$ progress is steady, with some drift\n",
    "* $\\dot{\\phi}_{t+1} = \\dot{\\phi}_{t} + \\sigma_y$ progress rate can drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def gesture_dynamics(prev_states):\n",
    "    # take an n x d array of particle samples\n",
    "    # return an n x d array representing the next states\n",
    "    # dummy code: apply no dynamics\n",
    "    next_states = np.array(prev_states)\n",
    "    next_states[:,5] += next_states[:,6] \n",
    "    \n",
    "    next_states[:,1] += np.random.normal(0,0.005,next_states[:,1].shape) # scale\n",
    "    \n",
    "    next_states[:,2:4] += np.random.normal(0,2.0,next_states[:,2:4].shape) # pos\n",
    "    next_states[:,4] += np.random.normal(0,0.5,next_states[:,4].shape) # angle\n",
    "    next_states[:,5] += np.random.normal(0,3.0,next_states[:,5].shape)    # phi\n",
    "    next_states[:,6] += np.random.normal(0,0.2,next_states[:,6].shape)  # dphi\n",
    "    \n",
    "    return next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priors\n",
    "We then define our initial guesses for the state of the system, encoded as prior probability distributions.\n",
    "\n",
    "* $s_0$ $N(1,0.25)$, should be around original size, with some latitude\n",
    "* $x_0,y_0$ $U(0,\\text{max_screen_size})$, could be anywhere on screen\n",
    "* $\\theta_0$ $N(0,10)$, angle will be close to original, with std. dev. of ~15 degrees\n",
    "* $\\phi_0$ $N(0,0.1)$, gestures will begin close to their start\n",
    "* $\\dot{\\phi}_0$ N($\\mu_{\\dot{\\phi}}, \\sigma_{\\dot{\\phi}})$), gesture speeds will be distributed according to the observed speeds from the template\n",
    "* $i_0$ discrete $U(0,n-1)$, could be any gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gesture_prior(n):\n",
    "    # return an n x d matrix with columns [i, s, x_c, y_c, \\theta, \\phi, \\phi_dot] as an initial guess\n",
    "    # these should call a function draw a value from a distribution\n",
    "    # dummy code: choose a random class and set all other variables to 1.0\n",
    "    return np.stack([\n",
    "        np.random.randint(0,n_gestures,size=n), \n",
    "        np.random.normal(1.0,0.25,size=n), \n",
    "        np.random.uniform(-200,400,size=n), np.random.uniform(-200,400,size=n),\n",
    "        np.random.normal(0.0, 10.0, size=n), \n",
    "        np.random.normal(0.0, 10.0, size=n), \n",
    "        np.random.normal(1.0, 0.3, size=n)]).T\n",
    "\n",
    "# print some test samples from the prior\n",
    "print(gesture_prior(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting function\n",
    "We again use the simple heat kernel (or RBF):\n",
    "$$w_i = e^{\\left(-\\frac{(y-y^\\prime)^2}{2\\beta^2}\\right)}$$\n",
    "(the specific choice of weighting  function is rarely very important, except if there are particularly unusual states to be compared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def gesture_weight(hypothesized, true):\n",
    "    # take a 2D observation (x,y)\n",
    "    # and an n x 2 matrix of observation samples (returned from gesture_observation())\n",
    "    # return the weight for each, representing how similar they are\n",
    "    gesture_beta = 180.0          # the RBF width\n",
    "    \n",
    "    # RBF similarity function       \n",
    "    w = np.exp(-np.sum((hypothesized-true)**2, axis=1)/(0.5*gesture_beta**2))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gestures\n",
    "import imp\n",
    "gestures = imp.reload(gestures)\n",
    "gestures.interactive_recogniser(\n",
    "    dynamics=gesture_dynamics,\n",
    "    observation=gesture_observation,\n",
    "    prior=gesture_prior,\n",
    "    weight=gesture_weight,\n",
    "    gestures=g.gestures)\n",
    "%gui tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfalls\n",
    "Particle filter in particular can be tricky to tune. \n",
    "\n",
    "* As the state space expands (i.e. the number of variables tracked), the number of particles requires can increase. This makes it difficult to run filters efficiently in problems that have lots of degrees of freedom.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "---------------------\n",
    "### Scope and limitations\n",
    "#### Scope\n",
    "* Probabilistic filters can be applied to many problems in HCI. Typically, if a process unfolds over time and there is uncertainty, a probabilistic filter is a strong candidate for inference. \n",
    "\n",
    "* The fact that inference is performed over time is a potential advantage over \"static\" classification approaches, as feedback can be generated on the fly, instead only after completion of an action. \n",
    "\n",
    "* In the specific context of gestures, the ability to infer the start and end-point of gestures can solve the \"segmentation problem\" or \"gesture spotting problem\" that is often awkward and leads to kludges like button presses to segment actions.\n",
    "\n",
    "* Probabilistic motion models can easily be linked to higher-order probabilistic models which infer long-term actions on the part of the user. Because everything is a probability distribution, there is a simple common basis for integrating such models. This, for example, can include language models which estimate a distribution over text that is likely to be entered given both user input and a statistical model of language.\n",
    "\n",
    "#### Limitations\n",
    "* PFs can be computationally intensive to run. \n",
    "* Curse-of-dimensionality can make the attractive simplicity of PFs work poorly in practice as the state space expands (although often better than you might expect).\n",
    "* Sometimes the inverse probability model can be hard to formulate. Conversely, it is sometimes very much easier.\n",
    "* Particle filters are simple and elegant, but inferentially weak.\n",
    "* Kalman filters are rigid and restrictive, but very inferentially efficient.\n",
    "* Hybrid approaches (Ensemble Kalman filter, Unscented Kalman Filter, hybrid particle/Kalman filters, Rao-Blackwellized filters) can trade these qualities off, but they aren't off the shelf solutions (i.e. you need an expert!).\n",
    "\n",
    "\n",
    "### Resources\n",
    "#### Basic\n",
    "* Read the [Condensation paper](http://vision.stanford.edu/teaching/cs231b_spring1415/papers/isard-blake-98.pdf).\n",
    "* Read [the Kalman filter in pictures](http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/)\n",
    "* Watch [the particle filter without equations](https://www.youtube.com/watch?v=aUkBa1zMKv4)\n",
    "\n",
    "#### Advanced\n",
    "* [A technical but succinct and clear explanation of the particle filter](http://www.cns.nyu.edu/~eorhan/notes/particle-filtering.pdf)\n",
    "* [A bibliography of particle filter papers](http://www.stats.ox.ac.uk/~doucet/smc_resources.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Future of probabilistic filtering\n",
    "\n",
    "#### Learned models\n",
    "\n",
    "Much use of probabilistic filters has depended on strong mathematical models of the fundamental process. For example, in rocket science, sophisticated physics models were used to specify the Kalman filters used for stable control. \n",
    "\n",
    "However, it is becoming increasingly possible to **infer** these models from observations. Techniques such as deep learning (for example variational autoencoders or generative adversarial networks) make it possible to learn very sophisticated *generative models* from observations of\n",
    "data.  These models can be dropped into probabilistic filters to produce robust inferential engines for user interaction.\n",
    "\n",
    "##### Example\n",
    "As an illustrative example, we recently built a touch pose estimator to estimate the pose of a finger from a capacitive sensor array (as found on a touch screen). We trained DCNN to predict finger pose from sensor images (inverse model), a separate deconvolutional CNN to predict sensor images from finger poses (forward model) and then fused these using a particle filter.\n",
    "\n",
    "<img src=\"imgs/fwd_inv.png\">\n",
    "This combined gives substantial robustness, and we were able to introduce a simple dynamics model, which filters out completely implausible movements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
